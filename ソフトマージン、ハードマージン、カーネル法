import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# 2クラス分類データの作成
X, y = datasets.make_classification(
    n_samples=100, 
    n_features=2, 
    n_informative=2,   # 情報的特徴量
    n_redundant=0,     # 冗長な特徴量
    n_repeated=0,      # 繰り返し特徴量
    n_classes=2, 
    random_state=42
)

# データの標準化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# トレーニングデータとテストデータに分ける
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ハードマージン SVM（C=∞）
hard_margin_svm = SVC(C=np.inf, kernel='linear')
hard_margin_svm.fit(X_train, y_train)

# ソフトマージン SVM（C=1）
soft_margin_svm = SVC(C=1, kernel='linear')
soft_margin_svm.fit(X_train, y_train)

# カーネル法（RBFカーネル）
kernel_svm = SVC(C=1, kernel='rbf', gamma='auto')
kernel_svm.fit(X_train, y_train)

# 境界線のプロット
def plot_svm_boundary(clf, X, y, title):
    plt.figure(figsize=(8, 6))
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
    
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.Paired)
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

# ハードマージンSVMの境界線
plot_svm_boundary(hard_margin_svm, X_train, y_train, 'Hard Margin SVM (C=∞)')

# ソフトマージンSVMの境界線
plot_svm_boundary(soft_margin_svm, X_train, y_train, 'Soft Margin SVM (C=1)')

# カーネル法 SVM の境界線
plot_svm_boundary(kernel_svm, X_train, y_train, 'Kernel SVM (RBF Kernel)')
