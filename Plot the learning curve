import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression

# Generate data (回帰問題のデータを生成)
X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
# Split data into training and validation sets (トレーニングデータとバリデーションデータに分割)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model (モデルを定義)
model = Sequential([
    # Add a dense layer with L2 regularization (L2正則化付きの全結合層を追加)
    Dense(64, input_dim=X.shape[1], activation='relu', kernel_regularizer=l2(0.001)),
    # Add dropout to prevent overfitting (過学習を防ぐためにドロップアウトを追加)
    Dropout(0.2),
    # Add another dense layer (別の全結合層を追加)
    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.2),
    # Output layer for regression (回帰用の出力層)
    Dense(1)
])

# Compile the model with Adam optimizer (モデルをAdam最適化でコンパイル)
model.compile(
    optimizer=Adam(learning_rate=0.001),  # Set the learning rate (学習率を設定)
    loss='mse'  # Use mean squared error as the loss function (平均二乗誤差を損失関数として使用)
)

# Train the model (モデルの訓練)
history = model.fit(
    X_train, y_train,
    epochs=100,  # Number of epochs (エポック数)
    batch_size=32,  # Batch size (バッチサイズ)
    validation_data=(X_val, y_val),  # Validation data (バリデーションデータ)
    verbose=0  # Hide training output (訓練出力を非表示)
)

# Plot the learning curve (学習曲線をプロット)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Learning Curve')
plt.legend()
plt.grid(True)
plt.show()
