import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np

# 1. CNN (Convolutional Neural Network)
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.fc1 = nn.Linear(32 * 14 * 14, 128)  # 28x28 input, after pooling 14x14
        self.fc2 = nn.Linear(128, 10)  # 10 classes

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = x.view(-1, 32 * 14 * 14)  # Flatten the data
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 2. RNN (Recurrent Neural Network)
class SimpleRNN(nn.Module):
    def __init__(self, input_size=28, hidden_size=128, output_size=10):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), 128)  # Initial hidden state
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])  # Use the last hidden state
        return out

# 3. VAE (Variational Autoencoder)
class VAE(nn.Module):
    def __init__(self, z_dim=20):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(784, 400)
        self.fc21 = nn.Linear(400, z_dim)  # mean of z
        self.fc22 = nn.Linear(400, z_dim)  # log variance of z
        self.fc3 = nn.Linear(z_dim, 400)
        self.fc4 = nn.Linear(400, 784)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std

    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

# 4. GAN (Generative Adversarial Network)
class Generator(nn.Module):
    def __init__(self, z_dim=100):
        super(Generator, self).__init__()
        self.fc = nn.Linear(z_dim, 256)
        self.fc2 = nn.Linear(256, 512)
        self.fc3 = nn.Linear(512, 1024)
        self.fc4 = nn.Linear(1024, 784)
        self.final = nn.Tanh()

    def forward(self, z):
        x = F.relu(self.fc(z))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.final(self.fc4(x))
        return x

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.fc = nn.Linear(784, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 1)
        self.final = nn.Sigmoid()

    def forward(self, x):
        x = F.leaky_relu(self.fc(x), 0.2)
        x = F.leaky_relu(self.fc2(x), 0.2)
        x = F.leaky_relu(self.fc3(x), 0.2)
        x = self.final(self.fc4(x))
        return x

# DataLoader for MNIST
def load_data(batch_size=64):
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    return train_loader

# Training functions for each model
def train_cnn(model, train_loader, epochs=1):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    for epoch in range(epochs):
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item()}")

def train_rnn(model, train_loader, epochs=1):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    for epoch in range(epochs):
        for images, labels in train_loader:
            optimizer.zero_grad()
            images = images.view(-1, 28, 28)  # RNN expects sequences
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item()}")

def train_vae(model, train_loader, epochs=1):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    for epoch in range(epochs):
        for images, _ in train_loader:
            images = images.view(-1, 784)  # 28x28の画像を784次元にフラット化
            images = images * 0.5 + 0.5  # 入力画像を[0, 1]の範囲にスケール

            optimizer.zero_grad()
            recon_batch, mu, logvar = model(images)
            
            # バイナリクロスエントロピーを計算
            BCE = F.binary_cross_entropy(recon_batch, images, reduction='sum')
            
            # KLダイバージェンスの計算
            M = 1 + logvar - mu.pow(2) - logvar.exp()
            loss = BCE + torch.sum(M) * -0.5  # ELBO loss

            loss.backward()
            optimizer.step()
        
        print(f"Epoch {epoch+1}, Loss: {loss.item()}")

def train_gan(generator, discriminator, train_loader, epochs=1):
    criterion = nn.BCELoss()
    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    for epoch in range(epochs):
        for images, _ in train_loader:
            # Train Discriminator
            real_labels = torch.ones(images.size(0), 1)
            fake_labels = torch.zeros(images.size(0), 1)

            optimizer_D.zero_grad()
            outputs = discriminator(images.view(-1, 784))
            loss_D_real = criterion(outputs, real_labels)
            loss_D_real.backward()

            z = torch.randn(images.size(0), 100)
            fake_images = generator(z)
            outputs = discriminator(fake_images.detach())
            loss_D_fake = criterion(outputs, fake_labels)
            loss_D_fake.backward()
            optimizer_D.step()

            # Train Generator
            optimizer_G.zero_grad()
            outputs = discriminator(fake_images)
            loss_G = criterion(outputs, real_labels)
            loss_G.backward()
            optimizer_G.step()

        print(f"Epoch {epoch+1}, Loss D: {loss_D_real.item() + loss_D_fake.item()}, Loss G: {loss_G.item()}")

# Example usage:
if __name__ == "__main__":
    train_loader = load_data(batch_size=64)

    # CNN
    cnn_model = SimpleCNN()
    train_cnn(cnn_model, train_loader, epochs=1)

    # RNN
    rnn_model = SimpleRNN()
    train_rnn(rnn_model, train_loader, epochs=1)

    # VAE
    vae_model = VAE()
    train_vae(vae_model, train_loader, epochs=1)

    # GAN
    generator = Generator()
    discriminator = Discriminator()
    train_gan(generator, discriminator, train_loader, epochs=1)
