import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

# データの生成 (Generate sample data)
np.random.seed(42)
n_samples = 300
mean1 = [0, 0]
cov1 = [[1, 0.5], [0.5, 1]]
mean2 = [3, 5]
cov2 = [[1, -0.3], [-0.3, 1]]
X1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2)
X2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2)
X = np.vstack([X1, X2])

# GMMの初期パラメータ (Initialize GMM parameters)
n_components = 2  # 混合ガウス分布の数 (Number of Gaussian components)
weights = np.ones(n_components) / n_components
means = np.array([[0, 0], [2, 2]])
covariances = np.array([np.eye(2) for _ in range(n_components)])

# EMアルゴリズムの設定 (EM algorithm settings)
n_iterations = 100
log_likelihoods = []

for iteration in range(n_iterations):
    # Eステップ: 各データポイントに対する各ガウス分布の責任度を計算 (E-step)
    responsibilities = np.zeros((n_samples, n_components))
    for k in range(n_components):
        responsibilities[:, k] = weights[k] * multivariate_normal.pdf(X, mean=means[k], cov=covariances[k])
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    
    # Mステップ: 重み、平均、共分散行列を更新 (M-step)
    N_k = responsibilities.sum(axis=0)
    weights = N_k / n_samples
    means = (responsibilities.T @ X) / N_k[:, np.newaxis]
    covariances = np.array([
        (np.dot((responsibilities[:, k:k+1] * (X - means[k])).T, (X - means[k])) / N_k[k])
        for k in range(n_components)
    ])
    
    # 対数尤度の計算 (Calculate log-likelihood)
    log_likelihood = np.sum(np.log(np.sum([
        weights[k] * multivariate_normal.pdf(X, mean=means[k], cov=covariances[k])
        for k in range(n_components)
    ], axis=0)))
    log_likelihoods.append(log_likelihood)
    
    # ログの出力 (Print log-likelihood)
    if iteration % 10 == 0:
        print(f'Iteration {iteration}: Log-likelihood = {log_likelihood}')

# 結果のプロット (Plot the result)
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=responsibilities.argmax(axis=1), cmap='viridis', alpha=0.5)
for k in range(n_components):
    plt.scatter(means[k, 0], means[k, 1], marker='x', color='red', s=100, label=f'Component {k+1} Mean')
plt.title('GMM Clustering with EM Algorithm')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid()
plt.legend()
plt.show()

# 対数尤度の推移をプロット (Plot the log-likelihood over iterations)
plt.figure(figsize=(8, 4))
plt.plot(log_likelihoods)
plt.title('Log-likelihood over iterations')
plt.xlabel('Iteration')
plt.ylabel('Log-likelihood')
plt.grid()
plt.show()
