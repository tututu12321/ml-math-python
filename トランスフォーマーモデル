import torch
import torch.nn as nn
import torch.optim as optim

# Define the Transformer model
class SimpleTransformer(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
        super(SimpleTransformer, self).__init__()
        
        # Embedding layer for input data
        self.embedding = nn.Embedding(input_dim, model_dim)
        
        # Transformer encoder layers
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=model_dim, 
                nhead=num_heads, 
                dim_feedforward=model_dim * 4  # Feedforward dimension
            ),
            num_layers=num_layers
        )
        
        # Output layer
        self.fc_out = nn.Linear(model_dim, output_dim)
    
    def forward(self, x):
        # Embedding the input
        x = self.embedding(x)
        
        # Apply Transformer Encoder
        x = self.transformer_encoder(x)
        
        # Use only the output of the last token (e.g., for classification tasks)
        x = x.mean(dim=1)  # Pooling the sequence output
        
        # Output layer
        x = self.fc_out(x)
        
        return x

# Hyperparameters
input_dim = 100  # Vocabulary size
model_dim = 128  # Dimension of the model (embedding size)
num_heads = 4    # Number of attention heads
num_layers = 2   # Number of Transformer layers
output_dim = 10  # Output dimension (e.g., number of classes)

# Create the model
model = SimpleTransformer(input_dim, model_dim, num_heads, num_layers, output_dim)

# Sample input: Batch size of 32, sequence length of 10
input_data = torch.randint(0, input_dim, (32, 10))  # Random input indices (batch_size, seq_len)

# Forward pass through the model
output = model(input_data)

# Print output
print("Model output shape:", output.shape)
