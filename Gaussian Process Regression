import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from scipy.optimize import minimize

# --- 1. サンプルデータの生成 ---
np.random.seed(1)
def f(x):
    """真の関数"""
    return np.sin(x) * x

# トレーニングデータと予測用データの準備
X_train = np.array([1, 3, 5, 6, 8]).reshape(-1, 1)
y_train = f(X_train).ravel() + np.random.normal(0, 0.5, X_train.shape[0])
X_pred = np.linspace(0, 10, 100).reshape(-1, 1)

# --- 2. ガウス過程回帰モデルの定義と学習 ---
# カーネルを定義（定数カーネル + RBFカーネル）
kernel = C(1.0, (1e-4, 1e1)) * RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e1))
gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1)
gpr.fit(X_train, y_train)

# 予測と信頼区間の計算
y_pred, sigma = gpr.predict(X_pred, return_std=True)

# --- 3. プロット: GPRの予測と信頼区間 ---
plt.figure(figsize=(10, 6))
plt.plot(X_pred, f(X_pred), 'r:', label=r'$f(x) = \sin(x) \cdot x$')
plt.plot(X_train, y_train, 'ro', label='Training Data')
plt.plot(X_pred, y_pred, 'b-', label='GPR Prediction')
plt.fill_between(X_pred.ravel(), y_pred - 1.96 * sigma, y_pred + 1.96 * sigma, alpha=0.2, color='blue', label='95% Confidence Interval')
plt.title('Gaussian Process Regression')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.grid(True)
plt.show()

# --- 4. ベイズ最適化のステップ ---
def objective(x):
    """目的関数を予測に基づいて評価する"""
    x = np.array(x).reshape(-1, 1)  # 入力を2Dアレイに変換
    y_pred, sigma = gpr.predict(x, return_std=True)
    return -y_pred + 1.96 * sigma  # 信頼区間の上限を評価

# ベイズ最適化による最適値の探索
result = minimize(objective, x0=np.array([5.0]), bounds=[(0, 10)], method='L-BFGS-B')
opt_x = result.x[0]
opt_y = f(np.array([opt_x]))

# プロット: 最適化結果の表示
plt.figure(figsize=(10, 6))
plt.plot(X_pred, f(X_pred), 'r:', label=r'$f(x) = \sin(x) \cdot x$')
plt.plot(X_train, y_train, 'ro', label='Training Data')
plt.plot(X_pred, y_pred, 'b-', label='GPR Prediction')
plt.fill_between(X_pred.ravel(), y_pred - 1.96 * sigma, y_pred + 1.96 * sigma, alpha=0.2, color='blue', label='95% Confidence Interval')
plt.axvline(opt_x, color='green', linestyle='--', label=f'Optimized x = {opt_x:.2f}')
plt.scatter(opt_x, opt_y, color='green', s=100, zorder=5, label=f'Optimized y = {opt_y[0]:.2f}')
plt.title('Bayesian Optimization using GPR')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.grid(True)
plt.show()
