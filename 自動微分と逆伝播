import numpy as np

# シグモイド関数とその導関数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

# ネットワークの設定
input_size = 2    # 入力層のニューロン数
hidden_size = 2   # 隠れ層のニューロン数
output_size = 1   # 出力層のニューロン数

# 重みの初期化
np.random.seed(42)
W1 = np.random.randn(input_size, hidden_size)  # 入力から隠れ層への重み
W2 = np.random.randn(hidden_size, output_size)  # 隠れ層から出力層への重み

# バイアスの初期化
b1 = np.zeros((1, hidden_size))
b2 = np.zeros((1, output_size))

# 入力データとターゲット（出力）
X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])  # XORデータ
y = np.array([[0], [1], [1], [0]])  # 正しい結果

# 順伝播 (Forward Propagation)
def forward_propagation(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)  # 隠れ層の出力
    Z2 = np.dot(A1, W2) + b2
    A2 = sigmoid(Z2)  # 出力層の出力
    return A1, A2

# 逆伝播 (Backward Propagation)
def backward_propagation(X, y, A1, A2, W1, W2, b1, b2, learning_rate=0.1):
    m = X.shape[0]  # バッチサイズ

    # 出力層の勾配
    dA2 = 2 * (A2 - y) / m  # 損失関数の微分
    dZ2 = dA2 * sigmoid_derivative(A2)
    dW2 = np.dot(A1.T, dZ2)
    db2 = np.sum(dZ2, axis=0, keepdims=True)

    # 隠れ層の勾配
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * sigmoid_derivative(A1)
    dW1 = np.dot(X.T, dZ1)
    db1 = np.sum(dZ1, axis=0, keepdims=True)

    # 重みとバイアスの更新
    W1 -= learning_rate * dW1
    W2 -= learning_rate * dW2
    b1 -= learning_rate * db1
    b2 -= learning_rate * db2

    # 勾配の表示
    return W1, W2, b1, b2, dW1, dW2, db1, db2

# トレーニングの実行
for epoch in range(10000):
    # 順伝播
    A1, A2 = forward_propagation(X, W1, b1, W2, b2)

    # 逆伝播
    W1, W2, b1, b2, dW1, dW2, db1, db2 = backward_propagation(X, y, A1, A2, W1, W2, b1, b2)

    # 1000回ごとに損失と勾配を表示
    if epoch % 1000 == 0:
        loss = np.mean((y - A2) ** 2)
        print(f"Epoch {epoch}, Loss: {loss}")
        print(f"dW1: {dW1}, dW2: {dW2}, db1: {db1}, db2: {db2}")

# 最終的な出力の確認
print("Final output after training:")
print(A2)
