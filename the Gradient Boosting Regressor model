import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Generate synthetic data (シンプルなデータを生成)
np.random.seed(42)
X = np.linspace(0, 10, 100).reshape(-1, 1)  # Input features (入力特徴量)
y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])  # Target values with noise (ノイズ付き目標値)

# Split the data into training and testing sets (データを訓練用とテスト用に分割)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the Gradient Boosting Regressor model (勾配ブースティング回帰モデルを作成)
gbr = GradientBoostingRegressor(
    n_estimators=100,  # Number of boosting stages (決定木の数)
    learning_rate=0.1,  # Step size shrinkage (学習率)
    max_depth=3,  # Maximum depth of each tree (各決定木の最大深さ)
    random_state=42
)

# Fit the model to the training data (モデルを訓練データに適合)
gbr.fit(X_train, y_train)

# Predict on the test set (テストデータで予測)
y_pred = gbr.predict(X_test)

# Calculate mean squared error (平均二乗誤差を計算)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse:.4f}")

# Plot the training data, test data, and the model's predictions (訓練データ、テストデータ、モデルの予測をプロット)
plt.figure(figsize=(8, 5))
plt.scatter(X_train, y_train, color='blue', label='Training data')
plt.scatter(X_test, y_test, color='green', label='Test data')
plt.plot(X, gbr.predict(X), color='red', linewidth=2, label='Model prediction')
plt.title('Gradient Boosting Regression')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()
