import numpy as np
import matplotlib.pyplot as plt

# シンプルな線形回帰モデル
# Simple linear regression model
class LinearRegression:
    def __init__(self):
        # 重みとバイアスの初期化（ランダムな値）
        # Initialize weights and bias (random values)
        self.weight = np.random.randn()
        self.bias = np.random.randn()
        # Adam用の初期パラメータ
        # Initial parameters for Adam optimization
        self.m_w = 0  # 勾配の一次モーメント（重み用）/ First moment of gradient (for weight)
        self.v_w = 0  # 勾配の二次モーメント（重み用）/ Second moment of gradient (for weight)
        self.m_b = 0  # 勾配の一次モーメント（バイアス用）/ First moment of gradient (for bias)
        self.v_b = 0  # 勾配の二次モーメント（バイアス用）/ Second moment of gradient (for bias)

    # 予測関数
    # Prediction function
    def predict(self, x):
        return self.weight * x + self.bias

    # 損失関数（平均二乗誤差）
    # Loss function (Mean Squared Error)
    def compute_loss(self, x, y):
        y_pred = self.predict(x)
        return np.mean((y - y_pred) ** 2)

    # 勾配の計算
    # Compute gradients
    def compute_gradients(self, x, y):
        y_pred = self.predict(x)
        # 重みとバイアスに対する勾配の計算
        # Calculate gradients for weight and bias
        grad_weight = -2 * np.mean(x * (y - y_pred))
        grad_bias = -2 * np.mean(y - y_pred)
        return grad_weight, grad_bias

    # パラメータの更新（Adamを使用）
    # Update parameters using Adam
    def update_parameters(self, grad_weight, grad_bias, learning_rate, beta1, beta2, epsilon, t):
        # モーメントの更新
        # Update moments
        self.m_w = beta1 * self.m_w + (1 - beta1) * grad_weight
        self.v_w = beta2 * self.v_w + (1 - beta2) * (grad_weight ** 2)
        self.m_b = beta1 * self.m_b + (1 - beta1) * grad_bias
        self.v_b = beta2 * self.v_b + (1 - beta2) * (grad_bias ** 2)

        # バイアス補正
        # Bias correction
        m_w_hat = self.m_w / (1 - beta1 ** t)
        v_w_hat = self.v_w / (1 - beta2 ** t)
        m_b_hat = self.m_b / (1 - beta1 ** t)
        v_b_hat = self.v_b / (1 - beta2 ** t)

        # パラメータの更新
        # Update the parameters
        self.weight -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)
        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)

# データの生成
# Generate data
np.random.seed(42)
x_train = np.linspace(-1, 1, 100)
y_train = 3 * x_train + 2 + np.random.randn(*x_train.shape) * 0.1  # y = 3x + 2 with noise

# モデルのインスタンス化
# Instantiate the model
model = LinearRegression()

# ハイパーパラメータ
# Hyperparameters
learning_rate = 0.01
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
n_epochs = 1000

# 損失の履歴を保存
# Store the loss history
loss_history = []

# Adamによるトレーニング
# Training using Adam optimizer
for epoch in range(1, n_epochs + 1):
    grad_weight, grad_bias = model.compute_gradients(x_train, y_train)
    model.update_parameters(grad_weight, grad_bias, learning_rate, beta1, beta2, epsilon, epoch)
    
    # 損失の計算
    # Calculate the loss
    loss = model.compute_loss(x_train, y_train)
    loss_history.append(loss)
    
    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss}')

# 学習結果の表示
# Display training results
plt.figure(figsize=(10, 5))

# 損失関数の履歴をプロット
# Plot the loss history
plt.subplot(1, 2, 1)
plt.plot(range(n_epochs), loss_history, label='Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over Epochs (Adam)')
plt.grid(True)
plt.legend()

# 学習結果のプロット（予測線）
# Plot training results (prediction line)
plt.subplot(1, 2, 2)
plt.scatter(x_train, y_train, label='Data', color='blue', alpha=0.6)  # データをプロット (Plot data)
plt.plot(x_train, model.predict(x_train), color='red', label='Learned Model')  # 学習したモデルをプロット (Plot learned model)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Data and Learned Linear Model')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# 学習されたパラメータを表示
# Display learned parameters
print(f"Learned weight: {model.weight}")
print(f"Learned bias: {model.bias}")
