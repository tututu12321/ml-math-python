import numpy as np
import matplotlib.pyplot as plt

# シンプルなニューラルネットワークモデル（1層の線形モデル）
# Simple neural network model (single-layer linear model)
class SimpleNeuralNetwork:
    def __init__(self):
        # 重みとバイアスの初期化（ランダムな小さな値）
        # Initialize weights and bias with small random values
        self.weight = np.random.randn()
        self.bias = np.random.randn()

    # 順伝播（予測関数）
    # Forward pass (prediction function)
    def predict(self, x):
        return self.weight * x + self.bias

    # 損失関数（平均二乗誤差）
    # Loss function (Mean Squared Error)
    def compute_loss(self, x, y):
        y_pred = self.predict(x)
        return np.mean((y - y_pred) ** 2)

    # 勾配を計算
    # Compute gradients
    def compute_gradients(self, x, y):
        y_pred = self.predict(x)
        # 重みとバイアスに関する勾配の計算
        # Calculate gradients for weight and bias
        grad_weight = -2 * np.mean(x * (y - y_pred))
        grad_bias = -2 * np.mean(y - y_pred)
        return grad_weight, grad_bias

    # パラメータの更新
    # Update parameters
    def update_parameters(self, grad_weight, grad_bias, learning_rate):
        self.weight -= learning_rate * grad_weight
        self.bias -= learning_rate * grad_bias

# データの生成（線形データ）
# Generate data (linear data)
np.random.seed(42)
x_train = np.linspace(-1, 1, 100)
y_train = 3 * x_train + 2 + np.random.randn(*x_train.shape) * 0.1  # y = 3x + 2 にノイズを加えたデータ (Data with noise added)

# モデルのインスタンス化
# Instantiate the model
model = SimpleNeuralNetwork()

# ハイパーパラメータ
# Hyperparameters
learning_rate = 0.1
n_epochs = 50

# 損失の履歴を保存
# Store loss history
loss_history = []

# 勾配降下法によるトレーニング
# Training using gradient descent
for epoch in range(n_epochs):
    # 勾配の計算
    # Calculate gradients
    grad_weight, grad_bias = model.compute_gradients(x_train, y_train)
    
    # パラメータの更新
    # Update parameters
    model.update_parameters(grad_weight, grad_bias, learning_rate)
    
    # 損失の計算
    # Calculate loss
    loss = model.compute_loss(x_train, y_train)
    loss_history.append(loss)
    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {loss}')

# 学習結果の表示
# Display training results
plt.figure(figsize=(10, 5))

# 損失関数の履歴をプロット
# Plot the loss history
plt.subplot(1, 2, 1)
plt.plot(range(n_epochs), loss_history, label='Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss over Epochs')
plt.grid(True)
plt.legend()

# 学習結果のプロット（予測線）
# Plot the training results (prediction line)
plt.subplot(1, 2, 2)
plt.scatter(x_train, y_train, label='Data', color='blue', alpha=0.6)  # データをプロット (Plot data)
plt.plot(x_train, model.predict(x_train), color='red', label='Learned Model')  # 学習したモデルをプロット (Plot learned model)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Data and Learned Linear Model')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# 学習されたパラメータを表示
# Display the learned parameters
print(f"Learned weight: {model.weight}")
print(f"Learned bias: {model.bias}")
