import numpy as np
import gymnasium as gym

# 環境の作成
env = gym.make("CartPole-v1")

# 状態の離散化のための区切り数
num_bins = (6, 6, 6, 6)  # 4次元状態空間をそれぞれ6分割

# 状態の範囲 (上限と下限)
state_bounds = list(zip(env.observation_space.low, env.observation_space.high))
state_bounds[1] = (-2.4, 2.4)  # カートの速度の制限
state_bounds[3] = (-2, 2)  # 棒の角速度の制限

# Qテーブルの初期化
Q_table = np.zeros(num_bins + (env.action_space.n,))

# ハイパーパラメータ
alpha = 0.1  # 学習率
gamma = 0.99  # 割引率
epsilon = 1.0  # 初期の探索率
epsilon_min = 0.01  # 最小探索率
epsilon_decay = 0.995  # 探索率の減衰

# 状態を離散化する関数
def discretize_state(state):
    """連続状態を離散状態に変換"""
    discretized = []
    for i in range(len(state)):
        bound_min, bound_max = state_bounds[i]
        ratio = (state[i] - bound_min) / (bound_max - bound_min)
        new_state = int(np.floor(ratio * num_bins[i]))
        new_state = min(num_bins[i] - 1, max(0, new_state))  # 範囲内に収める
        discretized.append(new_state)
    return tuple(discretized)

# 学習の実行
num_episodes = 2000
max_steps = 500

for episode in range(num_episodes):
    state = discretize_state(env.reset()[0])  # 初期状態の取得と離散化
    total_reward = 0

    for step in range(max_steps):
        # ε-greedy 方策で行動選択
        if np.random.rand() < epsilon:
            action = env.action_space.sample()  # ランダム行動 (探索)
        else:
            action = np.argmax(Q_table[state])  # 最適行動 (活用)

        # 環境を1ステップ進める
        next_state, reward, done, _, _ = env.step(action)
        next_state = discretize_state(next_state)
        total_reward += reward

        # Qテーブルの更新
        Q_table[state][action] += alpha * (reward + gamma * np.max(Q_table[next_state]) - Q_table[state][action])

        state = next_state  # 状態の更新

        if done:
            break

    # 探索率の減衰
    epsilon = max(epsilon_min, epsilon * epsilon_decay)

    # 進捗表示
    if episode % 100 == 0:
        print(f"Episode {episode}: Total Reward = {total_reward}, Epsilon = {epsilon:.4f}")

print("学習完了！")

# 学習後のテスト
num_test_episodes = 10
for i in range(num_test_episodes):
    state = discretize_state(env.reset()[0])
    total_reward = 0
    done = False
    while not done:
        action = np.argmax(Q_table[state])  # 学習済み Qテーブルから行動選択
        next_state, reward, done, _, _ = env.step(action)
        state = discretize_state(next_state)
        total_reward += reward
    print(f"Test Episode {i+1}: Total Reward = {total_reward}")

env.close()
