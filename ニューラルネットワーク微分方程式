import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torchdiffeq import odeint

# ニューラルネットワークによる微分方程式の定義
class ODEFunc(nn.Module):
    def __init__(self):
        super(ODEFunc, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 20),  # 1次元入力を20次元へ
            nn.Tanh(),
            nn.Linear(20, 1)   # 1次元出力
        )

    def forward(self, t, y):
        return self.net(y)  # dy/dt = f(y)

# 初期状態の設定
t = torch.linspace(0, 2, 100)  # 時間 0 から 2 まで 100 ステップ
y0 = torch.tensor([[1.0]])  # 初期値 y(0) = 1

# ODE の解を求める関数
def solve_ode(func, y0, t):
    with torch.no_grad():
        y = odeint(func, y0, t)  # 微分方程式の数値解を計算
    return y.numpy()

# モデルの学習
def train_ode(func, y0, t, num_epochs=1000, lr=0.01):
    optimizer = optim.Adam(func.parameters(), lr=lr)
    
    for epoch in range(num_epochs):
        optimizer.zero_grad()
        y_pred = odeint(func, y0, t)
        loss = torch.mean((y_pred - torch.exp(t)) ** 2)  # 目標関数 y = exp(t)
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')
    
    return func

# 学習の実行
func = ODEFunc()
trained_func = train_ode(func, y0, t)

# ODE の解を計算
y_pred = solve_ode(trained_func, y0, t)

# 解析解との比較
plt.figure(figsize=(8, 5))
plt.plot(t.numpy(), np.exp(t.numpy()), label='Analytical Solution (exp(t))', linestyle='dashed')
plt.plot(t.numpy(), y_pred[:, :, 0], label='Neural ODE Solution')
plt.xlabel("Time")
plt.ylabel("y")
plt.legend()
plt.title("Neural ODE Approximation of dy/dt = f(y)")
plt.show()
