import numpy as np
import matplotlib.pyplot as plt

# サンプルデータの生成
np.random.seed(0)
n = 50
x = np.linspace(0, 10, n)
y_true = 2 * x + 1
y = y_true + np.random.normal(0, 2, n)  # ノイズを加えたデータ

# 最小二乗法による直線フィッティング
X = np.vstack([np.ones(n), x]).T
beta = np.linalg.inv(X.T @ X) @ X.T @ y  # 回帰係数の計算
y_pred = X @ beta  # 予測値

# 各変動の計算
y_mean = np.mean(y)
total_variation = np.sum((y - y_mean) ** 2)  # 全変動
regression_variation = np.sum((y_pred - y_mean) ** 2)  # 回帰変動
residual_variation = np.sum((y - y_pred) ** 2)  # 残差変動

# 確認：全変動 = 回帰変動 + 残差変動
print("Total Variation =", total_variation)
print("Regression Variation =", regression_variation)
print("Residual Variation =", residual_variation)
print("Total = Regression + Residual:", np.isclose(total_variation, regression_variation + residual_variation))

# 決定係数の計算
r_squared = regression_variation / total_variation
print("R-squared =", r_squared)

# プロット
plt.figure(figsize=(10, 6))
plt.scatter(x, y, label="Data", color="blue", alpha=0.6)
plt.plot(x, y_pred, color="red", label="Fitted Line")
plt.axhline(y=y_mean, color="green", linestyle="--", label="Mean of y")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.title("Linear Regression with Least Squares")
plt.grid()
plt.show()
