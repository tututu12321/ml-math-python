import numpy as np

# 活性化関数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

# 損失関数（平均二乗誤差）
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# 順伝播
def forward_propagation(X, weights, bias, activation_func):
    z = np.dot(X, weights) + bias
    a = activation_func(z)
    return a, z

# 逆伝播（出力層）
def backward_propagation_output_layer(X, y, a, z, activation_func_derivative):
    m = X.shape[0]
    dz = a - y  # 誤差
    dw = (1 / m) * np.dot(X.T, dz)  # 勾配
    db = (1 / m) * np.sum(dz, axis=0, keepdims=True)  # バイアスの勾配
    return dw, db, dz

# 勾配降下法
def gradient_descent(X, y, weights, bias, activation_func, activation_func_derivative, learning_rate, epochs):
    for epoch in range(epochs):
        # 順伝播
        a, z = forward_propagation(X, weights, bias, activation_func)
        
        # 損失の計算
        loss = mean_squared_error(y, a)
        
        # 逆伝播（出力層）
        dw, db, dz = backward_propagation_output_layer(X, y, a, z, activation_func_derivative)
        
        # 重みとバイアスの更新（出力層）
        weights -= learning_rate * dw
        bias -= learning_rate * db
        
        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {loss}')

    return weights, bias

# データの生成
np.random.seed(42)
X = np.random.randn(100, 3)  # 入力データ（100サンプル、3特徴量）
y = np.random.randint(0, 2, (100, 1))  # 出力データ（二値分類）

# 初期パラメータ
weights = np.random.randn(X.shape[1], 1)  # 重み（ランダムに初期化）
bias = np.zeros((1, 1))  # バイアス（初期値0）

# 学習率とエポック数
learning_rate = 0.01
epochs = 1000

# ニューラルネットワークの学習
weights, bias = gradient_descent(X, y, weights, bias, sigmoid, sigmoid_derivative, learning_rate, epochs)

# 最終的な予測
a, _ = forward_propagation(X, weights, bias, sigmoid)
predictions = (a > 0.5).astype(int)  # 予測値（0または1に変換）

# 結果表示
print(f'Final Predictions: {predictions[:10]}')  # 最初の10個の予測を表示
